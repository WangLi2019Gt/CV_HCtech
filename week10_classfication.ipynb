{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "\n",
    "#config\n",
    "device = torch.device('cuda:0')\n",
    "batch_size=32\n",
    "lr=0.01\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data\n",
    "label=[]\n",
    "img=[]\n",
    "f1=open(\"./week10_dataset/0_annotation_train.txt\",'r', encoding='UTF-8')\n",
    "for line in f1:\n",
    "    dic = json.loads(line)\n",
    "    label.append(dic[\"annotation\"][0][\"name\"])\n",
    "    img.append('./week10_dataset'+dic[\"source\"].split(\"image\")[-1])\n",
    "\n",
    "def train_data_generator(imgs, labels,classes, batch_size=32, img_size=(224,224)):\n",
    "    batch_index = np.arange(0, len(imgs))\n",
    "    img_out=[]\n",
    "    label_out=[]\n",
    "    while True:\n",
    "        np.random.shuffle(batch_index)\n",
    "        for i in batch_index:\n",
    "            if os.path.exists(imgs[i]):\n",
    "                img =cv2.imdecode(np.fromfile(imgs[i],dtype=np.uint8),1)\n",
    "                label_out.append(classes.index(labels[i]))\n",
    "                train_img= cv2.resize(img, (img_size[0], img_size[1]), interpolation=cv2.INTER_LINEAR)\n",
    "                img_out.append(train_img)\n",
    "                if len(img_out)>=batch_size:\n",
    "                    img_out = np.array(img_out, dtype=np.float32)\n",
    "                    img_out = img_out[:, :, :, ::-1]\n",
    "                    img_out=torch.from_numpy(np.array(img_out))\n",
    "                    img_out = img_out.permute(0, 3, 1, 2).float() / (255.0 / 2) - 1\n",
    "                    label_out = np.array(label_out, dtype=np.int64)\n",
    "                    label_out=torch.from_numpy(np.array(label_out))\n",
    "                    label_out = label_out.long()\n",
    "                    yield img_out, label_out\n",
    "                    img_out, label_out=[], []\n",
    "            else:\n",
    "                print(imgs[i], 'not exist')\n",
    "\n",
    "C = collections.Counter(label)\n",
    "classes=list(C.keys())\n",
    "data_generator=train_data_generator(img, label,classes, batch_size=batch_size, img_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "fc_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(fc_features, len(classes)).to(device)\n",
    "optimizer = torch.optim.Adam([model.fc.weight,model.fc.bias], lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:[epoch-0 , iter-1]  Loss: 2.746 | Acc: 0.000% (0/32)\n",
      "TRAIN:[epoch-0 , iter-2]  Loss: 2.855 | Acc: 6.250% (4/64)\n",
      "TRAIN:[epoch-0 , iter-3]  Loss: 3.211 | Acc: 8.333% (8/96)\n",
      "TRAIN:[epoch-0 , iter-4]  Loss: 3.390 | Acc: 12.500% (16/128)\n",
      "TRAIN:[epoch-1 , iter-1]  Loss: 2.902 | Acc: 34.375% (11/32)\n",
      "TRAIN:[epoch-1 , iter-2]  Loss: 2.878 | Acc: 34.375% (22/64)\n",
      "TRAIN:[epoch-1 , iter-3]  Loss: 2.480 | Acc: 40.625% (39/96)\n",
      "TRAIN:[epoch-1 , iter-4]  Loss: 2.356 | Acc: 44.531% (57/128)\n",
      "TRAIN:[epoch-2 , iter-1]  Loss: 1.827 | Acc: 81.250% (26/32)\n",
      "TRAIN:[epoch-2 , iter-2]  Loss: 1.530 | Acc: 78.125% (50/64)\n",
      "TRAIN:[epoch-2 , iter-3]  Loss: 1.543 | Acc: 70.833% (68/96)\n",
      "TRAIN:[epoch-2 , iter-4]  Loss: 1.342 | Acc: 72.656% (93/128)\n",
      "TRAIN:[epoch-3 , iter-1]  Loss: 1.087 | Acc: 53.125% (17/32)\n",
      "TRAIN:[epoch-3 , iter-2]  Loss: 1.012 | Acc: 57.812% (37/64)\n",
      "TRAIN:[epoch-3 , iter-3]  Loss: 1.005 | Acc: 61.458% (59/96)\n",
      "TRAIN:[epoch-3 , iter-4]  Loss: 0.968 | Acc: 64.062% (82/128)\n",
      "TRAIN:[epoch-4 , iter-1]  Loss: 0.571 | Acc: 78.125% (25/32)\n",
      "TRAIN:[epoch-4 , iter-2]  Loss: 0.705 | Acc: 75.000% (48/64)\n",
      "TRAIN:[epoch-4 , iter-3]  Loss: 0.646 | Acc: 77.083% (74/96)\n",
      "TRAIN:[epoch-4 , iter-4]  Loss: 0.612 | Acc: 78.906% (101/128)\n",
      "TRAIN:[epoch-5 , iter-1]  Loss: 0.327 | Acc: 96.875% (31/32)\n",
      "TRAIN:[epoch-5 , iter-2]  Loss: 0.381 | Acc: 93.750% (60/64)\n",
      "TRAIN:[epoch-5 , iter-3]  Loss: 0.378 | Acc: 91.667% (88/96)\n",
      "TRAIN:[epoch-5 , iter-4]  Loss: 0.391 | Acc: 90.625% (116/128)\n",
      "TRAIN:[epoch-6 , iter-1]  Loss: 0.209 | Acc: 96.875% (31/32)\n",
      "TRAIN:[epoch-6 , iter-2]  Loss: 0.292 | Acc: 93.750% (60/64)\n",
      "TRAIN:[epoch-6 , iter-3]  Loss: 0.271 | Acc: 93.750% (90/96)\n",
      "TRAIN:[epoch-6 , iter-4]  Loss: 0.231 | Acc: 95.312% (122/128)\n",
      "TRAIN:[epoch-7 , iter-1]  Loss: 0.173 | Acc: 96.875% (31/32)\n",
      "TRAIN:[epoch-7 , iter-2]  Loss: 0.177 | Acc: 95.312% (61/64)\n",
      "TRAIN:[epoch-7 , iter-3]  Loss: 0.187 | Acc: 95.833% (92/96)\n",
      "TRAIN:[epoch-7 , iter-4]  Loss: 0.178 | Acc: 96.875% (124/128)\n",
      "TRAIN:[epoch-8 , iter-1]  Loss: 0.151 | Acc: 96.875% (31/32)\n",
      "TRAIN:[epoch-8 , iter-2]  Loss: 0.150 | Acc: 96.875% (62/64)\n",
      "TRAIN:[epoch-8 , iter-3]  Loss: 0.130 | Acc: 97.917% (94/96)\n",
      "TRAIN:[epoch-8 , iter-4]  Loss: 0.135 | Acc: 98.438% (126/128)\n",
      "TRAIN:[epoch-9 , iter-1]  Loss: 0.098 | Acc: 100.000% (32/32)\n",
      "TRAIN:[epoch-9 , iter-2]  Loss: 0.092 | Acc: 100.000% (64/64)\n",
      "TRAIN:[epoch-9 , iter-3]  Loss: 0.092 | Acc: 100.000% (96/96)\n",
      "TRAIN:[epoch-9 , iter-4]  Loss: 0.093 | Acc: 100.000% (128/128)\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epoch_size=int(len(img)/batch_size)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss=0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for iter in range(1, epoch_size + 1):\n",
    "        imgs, labels = next(data_generator)\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predicts = model(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predicts, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = predicts.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        print( 'TRAIN:[epoch-%d , iter-%d]  Loss: %.3f | Acc: %.3f%% (%d/%d)' % (epoch, iter,epoch_loss / iter, 100. * correct / total, correct, total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
